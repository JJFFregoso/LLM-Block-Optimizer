# LLM-Block-Optimizer
Using the basic transformer architecture, coding an augmented version of the attention block, which improves training speed and performance for arithmetic operations.
